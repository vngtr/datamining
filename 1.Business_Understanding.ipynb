{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Analytics im HR - Fluktuation frühzeitig erkennen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### 0. Abstract\n",
    "\n",
    "Im Rahmen dieser Projektarbeit soll mittels Machine Learning Methoden ein Modell entwickelt werden, welches anhand verschiedener Merkmale Vorhersagen zur Mitarbeiterfluktuation zulässt. Beispielhaft wurde hierfür ein Kaggle Datensatz mit hypothetischen Mitarbeiterdaten, unter anderem zur Demografie, Einkommen, Betriebszugehörigkeit und Arbeitszufriedenheit sowie der Information, ob das Unternehmen verlassen wurde oder nicht, herangezogen. Der Entwicklungsprozess folgt dabei dem gängigen Ansatz und Analysemodell CRISP-DM. Unter den gerechneten Machine Learning Modellen konnte das mittels xxxlinearer Regressionxxx mit einer Accuracy von xxx %  unter Betrachtung der Merkmale xxx die Tendenz eines Beschäftigten das Unternehmen zu verlassen am zuverlässigsten prognostizieren. Vor der Umsetzung bedarf es weiterer Überlegungen im Hinblick auf Datenmenge und -qualität. Diesbezüglich muss das Modell mit realen Mitarbeiterdaten evaluiert und weiterentwickelt werden.\n",
    "\n",
    "---\n",
    "\n",
    "*Von Vanessa Güttinger, Johannes Hänle, Annika Raff, Natalie Rudolf im Fach Data Mining & Visual Analytics*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Einleitung\n",
    "\n",
    "Reaktionen des Unternehmensmanagements auf Kündigungen seitens ihrer Mitarbeiter lauten häufig „die Guten gehen zuerst“ oder „Reisende soll man nicht aufhalten“. Zwar ist die Arbeitskräftefluktuation in Deutschland seit Jahren mit 31 bis 33 Prozent recht konstant und die wirtschaftliche Lage sowie die damit einhergehende Entspannung am Arbeitsmarkt deuten auf weniger Personalbewegung hin, dennoch bleiben die Kosten für Unternehmen im Zusammenhang mit dem Abgang qualifizierter Arbeitskräfte hoch (Monsef 2023). Umso wichtiger ist es für Unternehmen, auch angesichts des anhaltenden Fachkräftemangels, Mitarbeiterabgänge frühzeitig zu erkennen um entsprechende Maßnahmen einzuleiten. Aus diesem Bedarf ergibt sich das Ziel der vorliegenden Datenanalyse im Rahmen des Moduls Data Mining und Visual Analytics: Die Anwendung geeigneter Machine Learning-Methoden zur Prognose der Mitarbeiterfluktuation. Grundlage hierfür bietet der Kaggle-Datensatz „HR-Employee-Atrittion“ von IBM. Exemplarisch sollen diese Daten dazu genutzt werden, ein optimales Modell zu trainieren und auszuwählen, das später in den Unternehmen auf reale Daten angewandt werden kann. Um einen möglichst strukturierten Ablauf zu gewährleisten, folgen sowohl das Projekt als auch der vorliegende Bericht der Methodik des Prozessmodells für Data Mining CRISP-DM (siehe Abbildung 1). Dieses besteht aus sechs iterativen Phasen (Chapman et al. 2000) und hat sich in der Praxis -auch innerhalb agiler Teams- bewährt (Saltz et al. 2017).\n",
    "\n",
    "![Crisp-DM](Abbildungen/Crisp-DM.png \"Abbildung 1\")\n",
    "\n",
    "*Abbildung 1: Prozessmodell CRISP-DM (eigene Darstellung in Anlehnung an (Chapman et al., 2000))*\n",
    "\n",
    "Im ersten Schritt wird der Business Case bewertet. Wichtigster Aspekt dieser Phase ist die Festlegung des Projektziels sowie dessen Erfolgskriterien und Mehrwert. Außerdem soll hier durch die Darstellung des aktuellen Stand der Forschung ein Verständnis für die zugrundeliegenden Mechanismen geschaffen werden. Bevor die Daten entsprechend aufbereitet werden und eine Architektur vorgeschlagen wird, gilt es im Rahmen des Kapitels Data Understanding ein Verständnis für die Datengrundlage zu schaffen. In der vierten Phase werden verschiedene Modellierungstechniken in Abhängigkeit der Anforderungen und der Daten thematisiert. Bevor die Erkenntnisse in der letzten Phase im Zuge eines ersten Prototyps bereitgestellt werden, gilt es die Modelle anhand festgelegter Projektziele sowie Erfolgs- und Gütekriterien zu evaluieren und auszuwählen. Schlussendlich werden im Fazit noch einmal die relevantesten Ergebnisse in Hinblick auf Limitationen und Ausblick eingeordnet (Chapman et al. 2000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Business Understanding\n",
    "\n",
    "Der rasante Fortschritt in der Informationstechnologie und dem damit einhergehenden Anstieg der Daten haben zu einer Veränderung in der Arbeitsweise von Entscheidungsträger in Organisationen geführt. Allerdings gibt es noch immer wenige Best Practices (im Gegenteil zu anderen Bereichen wie Finance und Marketing), wie man die reichhaltigen Daten über Mitarbeiter für bessere Entscheidungen im Personalwesen nutzen kann. Eines der meist diskutierten Themen in diesem Zusammenhang ist die Mitarbeiterfluktuation und vor allem die frühe Detektion dieser (Ekawati 2019). Aufgrund dieser Erfordernisse ergeben sich für das vorliegende Projekt folgende Ziele:\n",
    "\n",
    ">*1) Untersuchung verschiedener prädiktiven Analysemethoden,*   \n",
    ">*2) die Identifizierung der relevanten Kriterien zur Vorhersage der Fluktuation,*  \n",
    ">*3) sowie die Auswahl eines optimalen Modells in Bezug auf Vorhersagegenauigkeit, Komplexität, Modellqualität und Umsetzbarkeit.*  \n",
    "\n",
    "Ferner ergeben sich für die erfolgreiche Umsetzung des Projektes im realen Setting folgende Kriterien:  \n",
    "\n",
    ">*1) Einfachheit: Das Modell soll für den Anwender möglichst einfach nachzuvollziehen und interpretierbar sein, um die Akzeptanz zu steigern.*  \n",
    ">*2) Genauigkeit: Das Modell soll möglichst genaue, konsistente und verlässliche Prognosen treffen.*  \n",
    ">*3) Aktualität: Durch Sammlung weiterer Daten und Lernfähigkeit des Modells sollen stets die Verfügbarkeit und Aktualität gewährleitet werden.*    \n",
    ">*4) Skalierbarkeit: Das verwendete Modell soll in der Lage sein, sich an wachsende Anforderungen anzupassen.*  \n",
    "\n",
    "Das nach diesen Kriterien bereitgestellte Modell soll es den Entscheidern in den Organisationen (insbesondere im Human Resource Management) ermöglichen, Maßnahmen sowohl zur Steigerung der Mitarbeiterbindung, als auch zur Verhinderung von arbeitnehmerseitigen Kündigungen auf Basis von verlässlichen Vorhersagen von Fluktuationsindikatoren zu treffen. \n",
    "\n",
    "Im Business Understanding ist es unerlässlich, sich zunächst mit dem aktuellen Stand der Forschung zu befassen und relevante Konstrukte wie die Mitarbeiterfluktuation darzustellen. Außerdem soll im Folgenden ein Verständnis für die angewandten Machine-Learning-Methoden sowie prädiktiver Analysen kurz vorgestellt werden. Aus Gründen der Veranschaulichung erfolgt die tiefergreifende Beschreibung der Modelle und eingesetzten Libraries im Rahmen des Modeling.  \n",
    "\n",
    "##### *2.1 Mitarbeiterfluktuation*\n",
    "Im allgemeinen kann Mitarbeiterfluktuation als die Rotation von Arbeitskräften auf dem Arbeitsmarkt, zwischen Firmen, Jobs und Berufen sowie zwischen Zuständen von Beschäftigung und Arbeitslosigkeit verstanden werden (Abbasi und Hollman 2000).Hom und Griffeth haben in ihrer Studie das sog. \"Unfolding Model\" der freiwilligen Fluktuation thematisiert und dabei mehr den Entscheidungsaspekt des Mitarbeiters in den Fokus gestellt. Basierend auf der Image-Theorie gehen die Autoren davon aus, dass freiwillige Fluktuation als Entscheidung zur Kündigung angesehen wird. Die Prämisse: Menschen haben zuvor die Gründe für das Verlassen analysiert (Palich 1995).Bis heute gibt es allerdings wenig Konsistenz in den Ergebnissen was die Absicht der Menschen bestimmt, eine Organisation zu verlassen. Im Rahmen eines Literatur Review hat Ongori (2007) die Erkenntnisse zusammengefasst. Folglich können die Gründe nach ihrer Art entweder den jobbezogenen oder organisationalen Faktoren zugeschrieben werden. Organisationale Instabilität beispielsweise kann zu letzterem gezählt werden. Laut Zuber (2001) sind Mitarbeiter eher gewollt ein Unternehmen zu verlassen, wenn das Arbeitsumfeld schwer zu vorhersagen ist und ein hohes Maß an Ineffizienz herrscht. Außerdem können ein quantitativ und kostengetriebener Ansatz zur Mitarbeiterführung einen negativen Einfluss auf die Mitarbeiterbindung haben (Booth und Hamer 2007). Ebenso wie mangelnde Anerkennung (Abassi et. al 2000). Ein starkes Kommunikationssystem hingegen kann das Bedürfnis nach Information stillen und somit positiv zu einer geringen Mitarbeiterfluktuation beitragen. Auch die Organisationsgröße und -stabilität können Prädiktoren für die Mitarbeiterfluktuation sein. Große Organisationen bieten häufig eine höhere Entlohnung sowie bessere Aufstiegsmöglichkeiten und tragen somit zur Mitarbeiterbindung bei (Idson und Feaster 1990) .Bezahlung und bezahlungsbezogene Variablen haben laut Griffeth et al. (2000) allerdings einen eher moderaten Einfluss auf die Fluktuation. Allerdings scheint die Beziehung auch abhängig von der Leistung des Mitarbeiters zu sein. So ist die Kündigungswahrscheinlichkeit bei High-Potentials höher, wenn sie nicht ausreichend belohnt werden.  \n",
    "\n",
    "Arbeitsbedingter Stress (Jobstress sowie durch Rollenambiguität bedingt) und Arbeitsunzufriedenheit sowie mangelndes organisationales Commitment (Mitarbeiterbindung) sind jobbezogene Faktoren und somit individuelle Entscheidungen, die zu einer arbeitnehmerseitigen Kündigung beitragen können. Auch persönliche Handlungsmacht bzw. Kontrollüberzeugung, also inwiefern ein Mitarbeiter glaubt, dass externe Faktoren die Ergebnisse und ihr Handeln beeinflussen, können einen Einfluss auf die Fluktuation nehmen (Firth et al. 2004). Zu den externen Faktoren zählt auch der Arbeitsmarkt und die Arbeitslosenquote. So argumentiert Trevor (2001) beispielsweise, dass lokale Arbeitslosenquoten in Wechselwirkung mit der Arbeitszufriedenheit die Fluktuation auf dem Markt vorhersagen können.\n",
    "\n",
    "Die Fluktuationsrate hat sich zu einem wichtigen KPI (Key Performance Indicator) für das Management entwickelt. Der Verlust eines Mitarbeiters stellt aus verschiedenen Gründen ein Problem dar:  \n",
    "\n",
    ">*1) Es ist schwierig, geeignete Ersatzkräfte für Mitarbeiter zu finden, insbesondere für solche mit hoher Erfahrung und speziellen Fähigkeiten.*  \n",
    ">*2) Es erfordert Zeit, Aufwand und Geld (häufig zwischen 1 und 5 Jahresgehälter des ausgeschiedenen Mitarbeiters), neue Mitarbeiter zu rekrutieren.*  \n",
    ">*3) Der Verlust eines Mitarbeiters wirkt sich negativ auf laufende Projekte und Dienstleistungen aus.*  \n",
    ">*4) Die Einarbeitung neuer Mitarbeitenden bündelt Ressourcen zeitlicher und finanzieller Natur.*   \n",
    ">*5) Der Verlust eines Mitarbeiters kostet Geld. Die jährlichen Fluktuationsraten können bis zu 12–15 % betragen (Saradhi und Palshikar 2011; Sesil 2014).*  \n",
    "\n",
    "In der vorliegenden Arbeit liegt der Fokus auf der freiwilligen Mitarbeiterfluktuation, also wenn Mitarbeiter aus eigenen Gründen eine Organisation verlassen. Beim vorliegenden Datensatz ist unklar ob es sich bei der Zielvariable um arbeitnehmerseitigen oder durch den Arbeitgeber initiierten Abgang des Beschäftigten handelt. Deshalb wird von freiwilliger Fluktuation ausgegangen. \n",
    "\n",
    "##### *2.2 Predictive Analytics* \n",
    "\n",
    "Da die Fluktuationsquote lediglich Aussagen über die Vergangenheit zulässt, ist die Vorhersage künftiger Mitarbeiterabgänge, also prädiktiver Analysen, zur Ableitung von Maßnahmen umso wichtiger. Hierbei ist anzumerken, dass in der Forschung häufig die Kündigungsabsicht und nicht die tatsächliche Mitarbeiterfluktuation untersucht wird. Zur Prognose der Fluktuation gelten Längsschnittdaten, wie sie in modernen HR Informationssystemen in großen Mengen vorhanden sind, als Goldstandard. Der Vorteil liegt hier in der Untersuchung von Trends und Berücksichtigung der zeitlichen Dynamik. In der vorliegenden Datenanalyse wird mangels Verfügbarkeit solcher Daten auf Querschnittdaten zurückgegriffen (Rombaut und Guerry 2020).Im allgemeinen umfassen Predictive Analytics (dt. Prädiktive Analysen) eine breite Palette von Techniken wie Statistik, Modellierung und Data Mining, die aktuelle und historische Fakten verwenden, um Vorhersagen über die Zukunft zu treffen (Fitzenz und Mattox 2014).Im Gegensatz dazu untersuchen BI-Technologien wie Dashboards und Reports in der Regel was in der Vergangenheit passiert ist und sind deduktiv. Ferner präsentieren Dashboards eine faktische Menge an Hypothesen in Form von Metriken und KPIs. Typischerweise sind das Informationen wie Personalbestand, Kranken- oder die bereits erwähnte Fluktuationsquote. Predictive Analytics hingegen ist induktiv, lässt also die Daten den Weg weisen (Sesil 2014).Ekawati (2019) untersuchte Studien, die sich mit der prädiktiven Analyse von Mitarbeiterfluktuation befassen. Innerhalb der Studien kamen verschiedene Methoden bzw. Algorithmen, darunter SVM, Naive Bayes, Radom Forest sowie Gradient Boosting Machines zum Einsatz. Im allgemeinen erzielten Support Vector Machines mitunter die zuverlässigsten Ergebnisse zur Vorhersage der Mitarbeiterfluktuation (Ekawati 2019). Aufgrund der einfacheren Interpretierbarkeit werden häufig auch Entscheidungsbäume eingesetzt (Dolatabadi und Keynia 2017).  \n",
    "\n",
    "Machine Learning-Modelle können generell zwei Kategorien zugeordnet werden:  \n",
    "   \n",
    ">*1) Supervised Learning (dt. überwachtes Lernen) und*   \n",
    ">*2) Unsupervised Learning (dt. unüberwachtes Lernen).*   \n",
    "\n",
    "Ersteres beschreibt den Prozess der Erstellung prädiktiver Modelle unter Verwendung eines historischen Datensatzes, der sowohl Eingabemerkmale (sog. Features) und bekannte Ausgabewerte (sog. Labels) enthält. Überwachte Lernansätze umfassen im wesentlichen Klassifikation, Regression und Zeitreihenanalysen. Beim Unsupervised Learning hingegen wird ein Modell auf Basis eines Datensatzes trainiert, der nur die Eingabemerkmale aber keine bekannten Ausgabewerte enthält (Eckerson 2007). Im vorliegenden Projekt ist die Zielvariable bekannt, weshalb nachfolgende Ansätze des Supervised Learning zum Einsatz kommen. \n",
    "\n",
    "##### *2.3 Entscheidungsbäume*\n",
    "\n",
    "Entscheidungsbäume (Decision Trees) können für Regressions- und Klassifikationsaufgaben eingesetzt werden. Das baumförmige Modell besteht aus Zweige, die Beobachtungen eines bestimmten Elements repräsentieren sowie Blättern. Letztere enthalten die Schlussfolgerungen über den Zielwert des Elements. Der Algorithmus bildet mit einem Trainingsdatensatz einen Baum, bei dem jeder Knoten durch ein Attribut repräsentiert wird und die Zweige die Attributwerte darstellen. Der Nachteil dieser Methode liegt in der Instabilität des Algorithmus. So können bereits kleine Änderungen in den Trainingsdaten zu großen Variationen in der Leistung des Modells führen (Breiman 2001). \n",
    "\n",
    "##### *2.4 Random Forest*\n",
    "\n",
    "Der Random Forest versucht das Problem dieser Schwankungen zu lösen, indem die Tendenz zum Overfitting (dt. Überanpassung) an den Trainingsdatensatz reduziert ist. Das Prinzip des Ensemble-Lernalgorithmus liegt in der Kombination mehrerer, unabhängig voneinander trainierten Entscheidungsbäume und der Anwendung von Bootstrap-Aggregation (auch Bagging genannt). Durch das Bagging werden die Entscheidungsbäume jeweils auf einen zufälligen Teil des Datensatzes trainiert und wieder zurückgelegt. Nebst der Robustheit gegenüber Overfitting liegen die Vorteile dieser Methode in der Genauigkeit. Allerdings leidet die Interpretierbarkeit durch die Komplexität des Modells (Breiman 2001).\n",
    "\n",
    "##### *2.5 Naïve Bayes*\n",
    "\n",
    "Für die prädiktive Analyse der Mitarbeiterflukatuation ebenfalls prädestinierte Klassifikationstechnik, ist der Naïve Bayes. Im Gegensatz zum Random Forest handelt es sich hierbei um einen probabilistischen Klassifikator, welcher auf dem Bayes' Theorem basiert und die a-posteriori-Wahrscheinlichkeiten berechnet. Der Naïve Bayes-Klassifikator funktioniert gut unter der Annahme dass die Merkmale (bei gegebener Klasse) innerhalb der Daten eine bedingte Unabhängigkeit bzw. eine hohe Entropie (Unsicherheit) aufweisen. In der Praxis ist diese Unabhängigkeit häufig nicht gegeben, was zu ungenaueren Vorhersagen führen kann. Sind die Zusammenhänge komplex und nicht-linear sind Algorithmen wie der Random Forest besser geeignet. Die Vorteile des Naïve Bayes liegen unter anderem in der Einfachheit und Effizienz. Außerdem ist er gut geeignet wenn viele Merkmale vorliegen (Rish 2001).\n",
    "\n",
    "##### *2.6 Support Vector Machine*\n",
    "\n",
    " Die Support Vector Machine (SVM) ist ein überwachter Lernalgorithmus, der versucht das Risiko von Overfitting zu verringern. Bei der SVM wird jedes Mitglied des Trainingsdatensatzes einer vor zwei Kategorien zugewiesen, sodass es sich um einen nicht-probabilistischen binären linearen Klassifikator handelt. Bei einer linearen Klassifikation werden die Eingabetrainingsdatenpunkte in einem Raum (Hyperplane) abgebildet, jeder mit einem anderen Klassenlabel, das durch eine klare Lücke voneinander getrennt ist. Neue Datenpunkte werden dann in denselben Raum abgebildet und als Teil einer Klasse vorhergesagt, je nachdem, auf welcher Seite der Lücke sie liegen. SVM kann aber auch nicht-lineare Klassifikationen durchführen. Nebst der Robustheit gegenüber Überanpassung liegt der Vorteil in der Effektivität in hochdimensionalen Räumen (also wenn die Anzahl der Merkmale größer als die Anzahl an Beobachtungen ist). Durch verschiedene Kernel-Funktionen (darunter linear, polynominal und sigmoid) sind SVMs flexibel auf verschiedene Datensätze und Problemtypen einsetzbar. Allerdings gestaltet sich die Modellauswahl mitunter durch das Hyperparameter-Tuning und die Kernel-Auswahl als komplex. Unter der Modellkomplexität leidet ebenfalls die Interpretierbarkeit (Cortes und Vapnik 1995).\n",
    "\n",
    "##### *2.7 XGBoost*\n",
    "\n",
    "Ein weiterer, auf Entscheidungsbäume basierender Lernalgorithmus ist der sogenannte XGBoost (eXtreme Gradient Boosting). XGBoost unterstützt sowohl die Lösung von Klassifikations- als auch Regressionsaufgaben. Ähnlich dem Random Forest Algorithmus wird die Vorhersagegenauigkeit durch Kombination verschiedener Entscheidungsbäume erhöht, allerdings werden diese nicht unabhängig voneinander erstellt. Zur schrittweisen Verbesserung der Modelle auf Basis der Residuen nutzt XGBoost Gradient Boosting. So wird die Verlustfunktion während des Trainingsprozesses optimiert. Der Algorithmus bietet außerdem zahlreiche Hyperparameter zur Verbesserung der Modellleistung. Darunter auch Regularisierungstechniken wie L1- und L2 um Overfitting zu vermeiden und die Modellkompelxität zu kontrollieren. Nebst Anpassungsmöglichkeiten liegen die Stärken von XGBoost vor allem in der Parallelisierung und somit im effizienten Umgang mit großen Datensätze. Obwohl der Algorithmus Mechanismen zur Reduzierung von Overfitting bietet, kann die Vielschichtigkeit des Hyperparameter-Tunings dennoch zu Überanpassung führen. Außerdem ist die Interpretierbarkeit aufgrund der Komplexität eingeschränkt (Chen et al. 2014).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Überleitung"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
