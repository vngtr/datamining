{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling \n",
    "\n",
    "#### Pakete laden\n",
    "\n",
    "Alle benötigten Pakete kommen aus der Scikit-Learn (kurz auch sklearn). Das ist eine Python Bibliothek mit der sich Machine Learning Anwendungen einfach umsetzen lassen. Zu den bekanntesten Voraussetzung für die Verwendung von sklearn zählen Numpy und SciPy.\n",
    "\n",
    "**train_test_split**: Um die Effektivität eines Machine-Learning-Modells zu überprüfen, wird der ursprüngliche Datensatz in zwei Sets aufgeteilt: ein Trainingsset und ein Testset. Das Trainingsset wird verwendet, um das Modell auf einem Teil der Daten zu fitten, d. h. zu trainieren. Das Testset wird verwendet, um die Leistung des Modells auf dem anderen Teil der Daten zu bewerten. \n",
    "\n",
    "**accuracy_score**: Berechnet die Genauigkeit des Modells, also den Anteil der korrekt klassifizierten Beispiele.\n",
    "\n",
    "**classification_report**: Erzeugt einen Bericht, der Präzision, Recall und F1-Score für jedes Label im Klassifikationsproblem berechnet.\n",
    "\n",
    "**confusion_matrix**: Erzeugt eine Verwirrungsmatrix, die die Leistung eines Klassifikators darstellt, indem sie die Anzahl der wahren und falschen Klassifikationen zeigt.   \n",
    "Eine Confusion Matrix visualisiert die Leistung eines Klassifikationsmodells, indem die tatsächlichen und vorhergesagten Klassifikationen verglichen werden:\n",
    "\n",
    "|               | Vorhergesagt Positiv      | Vorhergesagt Negativ     |\n",
    "|---------------|--------------|-------------|\n",
    "| Tatsächlich Positiv       |   TP         |   FN        |\n",
    "| Tatsächlich Negativ       |   FP         |   TN        |\n",
    "\n",
    "- **True Positives (TP)**: Die Anzahl der tatsächlichen positiven Fälle, die korrekt als positiv vorhergesagt wurden.\n",
    "- **False Positives (FP)**: Die Anzahl der tatsächlichen negativen Fälle, die fälschlicherweise als positiv vorhergesagt wurden.\n",
    "- **True Negatives (TN)**: Die Anzahl der tatsächlichen negativen Fälle, die korrekt als negativ vorhergesagt wurden.\n",
    "- **False Negatives (FN)**: Die Anzahl der tatsächlichen positiven Fälle, die fälschlicherweise als negativ vorhergesagt wurden.  \n",
    "\n",
    "**DecisionTreeClassifier / RandomForestClassifier**: Sie dienen dazu, Entscheidungsbaum-Modelle für Klassifikationsaufgaben zu erstellen und zu trainieren\n",
    "\n",
    "**GridSearchCV / RandomizedSearchCV**: Sie werden zur Hyperparameter-Optimierung verwendet. Es hilft dabei, die besten Hyperparameter für ein Modell zu finden, indem es systematisch verschiedene Kombinationen von Hyperparametern testet und bewertet.\n",
    "\n",
    "**plot_tree**: Wird zur grafischen Wiedergabe eines Entscheidungsbaums benötigt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, precision_recall_curve, roc_curve, precision_score, recall_score, f1_score,  mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    " \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import shap\n",
    "\n",
    "import math\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import sys\n",
    "\n",
    "from scipy.stats import uniform, randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input: Bereinigte Daten von Data_Understanding_Preparation\n",
    "Einlesen aus pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der Datensatz employee_data_transformed enthält den Datentyp \"object\", im Datensatz employee_data wurde der Datentyp \"object\" mittels One-Hot Encoding in Booleans umgewandelt\n",
    "\n",
    "employee_data_transformed = pd.read_pickle('../HR_Data_raw.pkl')\n",
    "employee_data = pd.read_pickle('../HR_Data_One_Hot_Encoded.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Überprüfen ob die Spalten und Daten wie erwartet bereinigt sind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vorbereitung\n",
    "\n",
    "Wir können einen Entscheidungsbaum relativ einfach dadurch trainieren, dass wir die Inputvariable X und die vorherzusagenden Klassen Y definieren und den Entscheidungsbaum aus Skicit-Learn darauf trainieren. \n",
    "\n",
    "1) Arrays, die aus dem zu teilenden Datensatz entnommen wurden.  \n",
    "Beim überwachten Lernen sind diese Arrays das Input-Array X, das aus den erklärenden Variablen in den Spalten besteht, und das Output-Array y, das aus der Zielvariablen besteht. In diese Falle ist Attrition unsere Zielvariable, da dies vorhergesagt werden soll.\n",
    "\n",
    "2) Split Trainings- und Testdaten\n",
    "\n",
    "2.1) Die Größe des Testsets (test_size) und die Größe des Trainingssets (train_size).  \n",
    "Die Größe jedes Satzes ist entweder eine Dezimalzahl zwischen 0 und 1, die einen Anteil des Datensatzes darstellt. Hier wählen wir als Trainingsgröße 30% unserers Datensatzes aus\n",
    "Hinweis: Es ist ausreichend, nur eines dieser Argumente zu setzen, das zweite ist komplementär dazu.\n",
    "\n",
    "2.2) Der random state (random_state).  \n",
    "Der random state ist eine Zahl, die steuert, wie der Pseudo-Zufallsgenerator die Daten aufteilt.  \n",
    "Hinweis: Wenn du eine ganze Zahl als random state wählst, werden die Daten bei jedem Aufruf der Funktion auf die gleiche Weise aufgeteilt. Dies macht den Code also reproduzierbar.   \n",
    "[Warum \"42\"?](https://www.42-gmbh.de/unternehmen/warum-42/): Die Zahl 42 ist ein Insider-Witz in der Programmier- und Wissenschaftsgemeinschaft und stammt aus dem Buch \"Per Anhalter durch die Galaxis\" von Douglas Adams. In diesem Buch ist 42 die Antwort auf die ultimative Frage „nach dem Leben, dem Universum und allem“. Diese Antwort errechnet der größte existierende Computer in 7,5 Mio. Jahren:\n",
    "![grafik.png](attachment:grafik.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aufteilen der Daten in Zielvariable X und Attribute Y.\n",
    "X = employee_data.drop('Attrition', axis=1)\n",
    "y = employee_data.Attrition\n",
    "\n",
    "#Aufteilen der Daten in Test und Trainingsdaten.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anwendung von SMOTE auf die Trainingsdaten\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Überprüfen der neuen Klassenverteilung\n",
    "print(\"Ursprüngliche Klassenverteilung:\\n\", y.value_counts())\n",
    "print(\"Neue Klassenverteilung:\\n\", pd.Series(y_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Evaluieren der verschiedenen Modelle\n",
    "def evaluation(clf, X_resampled, y_resampled, X_test, y_test, train=True):\n",
    "    if train:\n",
    "        pred = clf.predict(X_resampled)\n",
    "        clf_report = pd.DataFrame(classification_report(y_resampled, pred, output_dict=True))\n",
    "        print(\"Train Result:\\n================================================\")\n",
    "        print(f\"Accuracy Score: {accuracy_score(y_resampled, pred) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_resampled, pred)}\\n\")\n",
    "        \n",
    "    elif train==False:\n",
    "        pred = clf.predict(X_test)\n",
    "        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n",
    "        print(\"Test Result:\\n================================================\")        \n",
    "        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FeatureImportance testen auf originalen Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_data_raw = pd.read_csv('WA_Fn-UseC_-HR-Employee-Attrition.csv', sep=',')\n",
    "employee_data_raw.columns = employee_data_raw.columns.str.strip()\n",
    "\n",
    "# Datenvorbereitung\n",
    "categorical_col = []\n",
    "for column in employee_data_raw.columns:\n",
    "    if employee_data_raw[column].dtype == object:\n",
    "        categorical_col.append(column)\n",
    "for column in categorical_col:\n",
    "    employee_data_raw[column] = employee_data_raw[column].astype(\"category\").cat.codes\n",
    "employee_data_raw['Attrition'] = employee_data_raw['Attrition'].astype(\"category\").cat.codes\n",
    "\n",
    "# Datenaufteilung\n",
    "Xraw = employee_data_raw.drop('Attrition', axis=1).values  # Merkmale als NumPy-Array\n",
    "yraw = employee_data_raw['Attrition'].values               # Zielvariable als NumPy-Array\n",
    "Xraw_train, Xraw_test, yraw_train, yraw_test = train_test_split(Xraw, yraw, test_size=0.25, random_state=42)\n",
    "\n",
    "# Anwendung von SMOTE auf die Trainingsdaten\n",
    "smote = SMOTE(random_state=42)\n",
    "X_raw_resampled, y_raw_resampled = smote.fit_resample(Xraw_train, yraw_train)\n",
    "\n",
    "# Zählen der Anzahl der \"Yes\"-Labels (Attrition = 1) vor SMOTE\n",
    "num_yes_before = np.sum(y_train == 1)\n",
    "print(f\"Anzahl der 'Yes' (Attrition = 1) vor SMOTE: {num_yes_before}\")\n",
    "num_yes_after = np.sum(y_raw_resampled == 1)\n",
    "print(f\"Anzahl der 'Yes' (Attrition = 1) nach SMOTE: {num_yes_after}\")\n",
    "#Entscheidungsbaum\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_raw_resampled, y_raw_resampled)\n",
    "\n",
    "# Feature Wichtigkeit Roh-Daten ausgeben\n",
    "feature_importances = tree_clf.feature_importances_\n",
    "feature_names = employee_data_raw.drop('Attrition', axis=1).columns\n",
    "feature_importance_employee_data_raw = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\\n\", feature_importance_employee_data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Decision Trees und Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erste Klassifizierung mit Trainings- und Testdaten für den Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "evaluation(tree_clf, X_resampled,  y_resampled, X_test,y_test, train=True)\n",
    "evaluation(tree_clf, X_resampled,  y_resampled, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Feature Importance gibt an, nach welchen Variablen sich der Decision Tree für die Klassifizierung der Mitarbeitenden entscheidet. Mit viel Abstand wird dabei der MonthlyIncome als wichtigste Kennzahl bewertet. In etwas Abstand folgen dann EnvironmentSatisfaction, TotalWorkingYears, OverTime und DistanceFromHome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Wichtigkeit bereinigte Daten ausgeben\n",
    "feature_importances = tree_clf.feature_importances_\n",
    "feature_names = employee_data.drop('Attrition', axis=1).columns\n",
    "feature_importance_employee_data = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\\n\", feature_importance_employee_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entscheidungsbaum visualisieren\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(tree_clf, feature_names=feature_names, filled=True, fontsize=2)\n",
    "plt.title(\"Vollständiger Entscheidungsbaum\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausschnitt des Entscheidungsbaums visualisieren (z.B. nur bis Tiefe 3)\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(tree_clf, feature_names=feature_names, filled=True, fontsize=10, max_depth=3)\n",
    "plt.title(\"Ausschnitt des Entscheidungsbaums (Tiefe 3)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parametertuning**\n",
    "\n",
    "Tuning eines Einzelnen Entscheidungsbaums kann Ihnen helfen, ein Gefühl dafür zu bekommen, welche Parameterbereiche sinnvoll sind und wie der baum auf unterscheidliche Variablen reagiert. Dies kann indirekt Informationen für die Random Forest-Konfiguration liefern.\n",
    "\n",
    "DecisionTree Parameter:\n",
    "\n",
    "criterion{“gini”, “entropy”, “log_loss”}, default=”gini”   \n",
    "Die Funktion zur Messung der Qualität einer Aufteilung. Supported criteria are “gini” for the Gini impurity and “log_loss” and “entropy” both for the Shannon information gain. Unterstützte Kriterien sind \"gini\" für die Gini-Verunreinigung und \"entropy\" für den Informationsgewinn. Beim Informationsgewinn wird das Entropiemaß als Maß für die Unreinheit verwendet und ein Knoten wird so aufgeteilt, dass er den größten Informationsgewinn bietet. Die Gini-Unreinheit hingegen misst die Divergenzen zwischen den Wahrscheinlichkeitsverteilungen der Werte des Zielattributs und teilt einen Knoten so auf, dass er die geringste Unreinheit ergibt.\n",
    "\n",
    "splitter{“best”, “random”}, default=”best”   \n",
    "Die Strategie, die für die Auswahl des Splits an jedem Knoten verwendet wird. Unterstützte Strategien sind \"best\" für die Auswahl des besten Splits und \"random\" für die Auswahl des besten zufälligen Splits.\n",
    "\n",
    "max_depthint, default=None\n",
    "Die maximale Tiefe des Baums. Falls keine, werden die Knoten so lange erweitert, bis alle Blätter rein sind oder bis alle Blätter weniger als min_samples_split Stichproben enthalten.\n",
    "\n",
    "min_samples_splitint or float, default=2   \n",
    "Die Mindestanzahl von Stichproben, die erforderlich ist, um einen internen Knoten zu teilen.\n",
    "Wenn int, dann gilt min_samples_split als Mindestanzahl.\n",
    "Wenn float, dann ist min_samples_split ein Bruch und ceil(min_samples_split * n_samples) ist die Mindestanzahl von Stichproben für jeden Split.\n",
    "\n",
    "min_samples_leafint or float, default=1   \n",
    "Die Mindestanzahl von Stichproben, die erforderlich ist, um einen Blattknoten zu erreichen. Ein Aufteilungspunkt in beliebiger Tiefe wird nur berücksichtigt, wenn er mindestens min_samples_leaf Trainingsstichproben in jedem der linken und rechten Zweige hinterlässt. Dies kann zu einer Glättung des Modells führen, insbesondere bei Regressionen.\n",
    "Die Mindestanzahl von Stichproben, die für einen Blattknoten erforderlich sind. Ein Splitpunkt in beliebiger Tiefe wird nur berücksichtigt, wenn er mindestens min_samples_leaf Trainingsstichproben in jedem der linken und rechten Zweige hinterlässt. Dies kann zu einer Glättung des Modells führen, insbesondere bei Regressionen.\n",
    "Wenn int, dann wird min_samples_leaf als Mindestanzahl betrachtet.\n",
    "Wenn float, dann ist min_samples_leaf ein Bruch und ceil(min_samples_leaf * n_samples) ist die Mindestanzahl von Stichproben für jeden Knoten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manuelle Festlegung der Parameter\n",
    "manual_params = {\n",
    "    \"criterion\": \"gini\",        # \"gini\" oder \"entropy\" oder \"log_loss\"\n",
    "    \"splitter\": \"best\",         # \"best\" oder oder \"random\"\n",
    "    \"max_depth\": 10,            # Wertebereich ab 1\n",
    "    \"min_samples_split\": 2,     # Wertebereich ab 2\n",
    "    \"min_samples_leaf\": 2       # Wertebereich ab 1\n",
    "}\n",
    "\n",
    "# Entscheidung für das Modell mit den festgelegten Parametern\n",
    "tree_clf = DecisionTreeClassifier(random_state=42, **manual_params)\n",
    "tree_clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "evaluation(tree_clf, X_resampled, y_resampled, X_test, y_test, train=True)\n",
    "evaluation(tree_clf, X_resampled, y_resampled, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search Cross Validation**\n",
    "\n",
    "Aschließend folgt die Parameteroptimierung über GridSearchCV, um die optimale Kombination zu erhatlen.\n",
    "\n",
    "Die Kreuzvalidierung ist eine bekannte Technik im Bereich des Machine Learning und hilft zu verstehen, wie gut ein Modell auf ungesehene Daten reagiert. Sie kann genauso auch für das Hyperparameter Tuning verwendet werden, um verschiedene Parametereinstellungen zu testen und zu vergleichen. Die Cross Validation nutzt eine sogenannte Resampling-Technik bei der der Datensatz in mehrere Teilmengen unterteilt wird. Die verbreiteste Umsetzung ist dabei die k-fold Cross Validation, bei der insgesamt k Teilmengen gebildet werden. Dabei wird in jeder Iteration eine andere Menge als Validierungsdatensatz genutzt, um eine unabhängige Aussage über die Leistung des Modells treffen zu können. \n",
    "\n",
    "In vielen Fällen wird dafür der folgende Prozess genutzt: \n",
    "\n",
    "Partitionierung der Daten: Der Datensatz wird als erstes in die gleich großen Teilmengen gesplittet. Diese fungieren während dem Trainings entweder als Trainings- oder Validierungssatz. Die unterschiedlichen Validierungssätze können dann genutzt werden, um die Modellleistung unabhängig bewerten und vergleichen zu können.   \n",
    "\n",
    "Auswahl von Hyperparametern: Vor dem Training wird ein Satz an Hyperparametern festgelegt, der in dieser Iteration getestet werden soll. Dies wurde durch die manuellen Anapssungen der Parameter erzielt.\n",
    "\n",
    "Aggregieren der Ergebnisse: Anschließend können die Ergebnisse zusammengefasst werden, um ein optimales Set an Hyperparameter zu bestimmen.\n",
    "\n",
    "Als optimal wird dabei nicht unbedingt die Auswahl betrachtet, die den höchsten Accuracy Score hat, sondern, die auch in den weiteren Kennzahlen performt. Dadurch wird zum Beispiel eine ungleichverteilung der Klassen vermieden, sodass die Vorhersage nicht meist auf \"Mitarbeitende bleiben im Unternehmen\" hinausläuft. Diese \"Falschaussage\" ist problematisch, da die Grundannahme der Personalabteilung lautet, dass die Mitarbeitenden bleiben und somit potenzielle Abgänge nicht erkannt werden, die durch eine korrekte Vorhersage verhindert hätten werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"criterion\":(\"gini\", \"entropy\", \"log_loss\"), \n",
    "    \"splitter\":(\"best\", \"random\"), \n",
    "    \"max_depth\":(list(range(1, 20))), \n",
    "    \"min_samples_split\":[2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "    \"min_samples_leaf\":list(range(1, 20)), \n",
    "}\n",
    "\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_cv = GridSearchCV(\n",
    "    tree_clf, \n",
    "    params, \n",
    "    scoring=\"f1\", \n",
    "    n_jobs=-1, \n",
    "    verbose=1, \n",
    "    cv=5\n",
    ")\n",
    "\n",
    "tree_cv.fit(X_resampled, y_resampled)\n",
    "best_params = tree_cv.best_params_\n",
    "print(f\"Best paramters: {best_params})\")\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(**best_params)\n",
    "tree_clf.fit(X_resampled, y_resampled)\n",
    "evaluation(tree_clf, X_resampled, y_resampled, X_test, y_test, train=True)\n",
    "evaluation(tree_clf, X_resampled, y_resampled, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erste Klassifizierung mit Trainings- und Testdaten für Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "rf_clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "evaluation(tree_clf, X_resampled, y_resampled, X_test, y_test, train=True)\n",
    "evaluation(tree_clf, X_resampled, y_resampled, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahieren der Feature Importance\n",
    "feature_importances = rf_clf.feature_importances_\n",
    "features = X_resampled.columns\n",
    "\n",
    "# Erstellen eines DataFrames für die Darstellung\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "\n",
    "# Sortieren nach Wichtigkeit\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Visualisierung der Feature Importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "plt.title('Feature Importance - Random Forest')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parametertuning**\n",
    "\n",
    "Für Random Forests ist es üblich, GridSearchCV oder RandomizedSearchCV zu verwenden, um die optimale Kombinationen von Hyperparametern zu verwenden und das beste Modell auszuwählen.\n",
    "\n",
    "Random Forest Parameter:\n",
    "\n",
    "n_estimators: Die Anzahl der Bäume im Wald.\n",
    "\n",
    "criterion: Die Funktion zur Messung der Qualität einer Aufteilung. Unterstützte Kriterien sind \"gini\" für die Gini-Unreinheit und \"entropy\" für den Informationsgewinn.\n",
    "\n",
    "max_depth: Die maximale Tiefe des Baums. Falls keine, werden die Knoten so lange expandiert, bis alle Blätter rein sind oder bis alle Blätter weniger als min_samples_split samples enthalten.\n",
    "\n",
    "min_samples_split: Die Mindestanzahl von Stichproben, die erforderlich ist, um einen internen Knoten zu teilen.\n",
    "\n",
    "min_samples_leaf: Die Mindestanzahl von Stichproben, die erforderlich ist, um einen Blattknoten zu erreichen. Ein Aufteilungspunkt in beliebiger Tiefe wird nur berücksichtigt, wenn er mindestens min_samples_leaf Trainingsstichproben in jedem der linken und rechten Zweige hinterlässt. Dies kann zu einer Glättung des Modells führen, insbesondere bei Regressionen.\n",
    "\n",
    "min_weight_fraction_leaf: Der minimale gewichtete Anteil der Gesamtsumme der Gewichte (aller Eingabestichproben), der an einem Blattknoten liegen muss. Wenn sample_weight nicht angegeben wird, haben die Stichproben das gleiche Gewicht.\n",
    "\n",
    "max_features: Die Anzahl der Merkmale, die bei der Suche nach dem besten Split berücksichtigt werden solle\n",
    "\n",
    "max_leaf_nodes: Wachsen lassen eines Baumes mit max_leaf_nodes in best-first fashion. Die besten Knoten werden als relative Verringerung der Unreinheit definiert. Wenn Keine, dann unbegrenzte Anzahl von Blattknoten.\n",
    "\n",
    "min_impurity_decrease: Ein Knoten wird geteilt, wenn dieser Split eine Verringerung der Unreinheit von mindestens diesem Wert bewirkt.\n",
    "\n",
    "min_impurity_split: Schwellenwert für das vorzeitige Anhalten des Baumwachstums. Ein Knoten wird geteilt, wenn seine Unreinheit über dem Schwellenwert liegt, andernfalls bleibt er ein Blatt.\n",
    "\n",
    "bootstrap: Ob bei der Erstellung von Bäumen Bootstrap-Stichproben verwendet werden. Wenn False, wird der gesamte Datensatz zur Erstellung jedes Baums verwendet.\n",
    "\n",
    "oob_score: Ob Out-of-Bag-Stichproben verwendet werden sollen, um die Generalisierungsgenauigkeit zu schätzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "random_grid = {\n",
    "    'n_estimators': n_estimators, \n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth, \n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf, \n",
    "    'bootstrap': bootstrap\n",
    "}\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf_cv = RandomizedSearchCV(\n",
    "    estimator=rf_clf, \n",
    "    scoring='f1',\n",
    "    param_distributions=random_grid, \n",
    "    n_iter=200, \n",
    "    cv=5, \n",
    "    verbose=1, \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_cv.fit(X_resampled, y_resampled)\n",
    "rf_best_params = rf_cv.best_params_\n",
    "print(f\"Best paramters: {rf_best_params})\")\n",
    "\n",
    "rf_clf = RandomForestClassifier(**rf_best_params)\n",
    "rf_clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "evaluation(tree_clf, X_resampled, y_resampled, X_test, y_test, train=True)\n",
    "evaluation(tree_clf, X_resampled, y_resampled, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnen der ROC-Kurve und AUC für den RandomForestClassifier\n",
    "y_prob_rf = rf_clf.predict_proba(X_test)[:, 1]\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)\n",
    "roc_auc_rf = roc_auc_score(y_test, y_prob_rf)\n",
    "\n",
    "# Plotten der ROC-Kurve für RandomForestClassifier\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(fpr_rf, tpr_rf, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_rf:0.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - RandomForestClassifier')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein weiterer, auf Entscheidungsbäume basierender Lernalgorithmus ist der sogenannte XGBoost (eXtreme Gradient Boosting). XGBoost unterstützt sowohl die Lösung von Klassifikations- als auch Regressionsaufgaben. Ähnlich dem Random Forest Algorithmus wird die Vorhersagegenauigkeit durch Kombination verschiedener Entscheidungsbäume erhöht, allerdings werden diese nicht unabhängig voneinander erstellt. Zur schrittweisen Verbesserung der Modelle auf Basis der Residuen nutzt XGBoost Gradient Boosting. So wird die Verlustfunktion während des Trainingsprozesses optimiert. Der Algorithmus bietet außerdem zahlreiche Hyperparameter zur Verbesserung der Modellleistung. Darunter auch Regularisierungstechniken wie L1- und L2 um Overfitting zu vermeiden und die Modellkompelxität zu kontrollieren. Nebst Anpassungsmöglichkeiten liegen die Stärken von XGBoost vor allem in der Parallelisierung und somit im effizienten Umgang mit großen Datensätze. Obwohl der Algorithmus Mechanismen zur Reduzierung von Overfitting bietet, kann die Vielschichtigkeit des Hyperparameter-Tunings dennoch zu Überanpassung führen. Außerdem ist die Interpretierbarkeit aufgrund der Komplexität eingeschränkt (Chen et al. 2014)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Daten für das Training vorbereiten:\n",
    "\n",
    "1. Zielvariable zunächst als \"category\" und dann in numerische Werte (binär in 0 und 1) umwandeln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen und Trainieren des XGBoost Modells\n",
    "\n",
    "# 1. Zielvariable in numerische Codes umwandeln (Yes --> 1, No --> 0)\n",
    "if y_resampled.dtype == 'object':\n",
    "    y_resampled = y_resampled.astype('category')\n",
    "    y_resampled = y_resampled.cat.codes\n",
    "\n",
    "if y_test.dtype == 'object':\n",
    "    y_test = y_test.astype('category')\n",
    "    y_test = y_test.cat.codes\n",
    "\n",
    "if y.dtype == 'object':\n",
    "    y = y.astype('category')\n",
    "    y = y.cat.codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Hyperparameter definieren und Modell trainieren:\n",
    "\n",
    "Zur Festlegung der Hyperparameter stehen unterschiedliche Ansätze zur Verfügung. Darunter **Grid Search** und **Random Search** (s. Gliederungspunkt 2.3)\n",
    "Zunächst wird das Modell allerdings nur mit den folgenden 5 Parametern trainiert, die auf einen Standard-Wert festgelgt werden:\n",
    "\n",
    "- *\"objective\"*: binäres Klassifikationsproblem mit einer logistischen Regression.\n",
    "- *\"max_depth\"*: Maximale Tiefe jedes Entscheidungsbaum. Ein höherer Wert ermöglich komplexere Werte (Achtung: Overfitting).\n",
    "- *\"learning_rate\"*: legt die Lernrate fest und steuert die Schrittweite des Gradientabstiegs.Ziel des Gradientenabstiegs ist das Finden der optimalen Werte der Modellparameter zur Minimierung der **Loss Function** (Misst Fehler zwischen vorhergesagten und tatsächlichen Werten, z.B. Mean Squared Error bei linearen Regression oder Log-Loss bei binären Klassifikation). Ein niedriger Werte macht Modell robust ggü. Overfitting (Achtung: mehr Iterationen nötig).\n",
    "- *\"n_estimators\"*: Anzahl der Boosting-Runden. Jede Boosting-Runde (sequenzielles Training) fügt neuen Entscheidungsbaum hinzu und korrigiert die Fehler (Residuen) des vorherigen Baumes. Idee: Kombination vieler schwacher Modelle ergibt starkes Lernmodell.\n",
    "- *\"eval_metric\"*: Metrik zur Evaluierung des Modells. \"logloss\" steht für logarithmischer Verlust (Logarithmic Loss) und bewertet wie gut das Klassifikationsmodell Wahrscheinlichkeiten für die Klassen vorhersagt. Beispiel einer \"guten\" Vorhersage: Tatsächliches Label ist 1 und Modell sagt Wahrscheinlichkeit von 0,9 vorher. Dann ist Log Loss relativ klein. Hier wird \"logloss\" als Evaluationsmetrik verwendet um Leistung des Modells während des Trainings zu bewerten (Ziel: Minimierung des logarithmischen Verlustes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen und Trainieren des XGBoost-Modells mit XGBClassifier\n",
    "xgb_model = XGBClassifier(objective='binary:logistic', max_depth=6, learning_rate=0.3, n_estimators=100, eval_metric='logloss')\n",
    "# xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Vorhersagen mit dem Modell\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "xgb_y_pred = (xgb_preds > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, xgb_y_pred)\n",
    "plt.figure(figsize=(4.5, 3))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Stay', 'Quit'], yticklabels=['Stay', 'Quit'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(xgb_model, X_resampled, y_resampled, X_test, y_test, train=True )\n",
    "evaluation(xgb_model, X_resampled, y_resampled, X_test, y_test, train=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3 Hyperparameter Tuning\n",
    "\n",
    "Mit Hilfe von GridSearchCV kann aus einer Auswahl von Parametern die ideale Kombination ermittelt werden. \n",
    "Durch Verwendung von RandomSearchCV kann durch die Festlegung von Iterationen eine zufällig ideale Parameterkombination ermittelt werden.\n",
    "\n",
    "- *max_depth:* Steuert, wie tief jeder einzelne Entscheidungsbaum wachsen kann. Größere Werte können zu komplexeren Modellen führen.\n",
    "- *learning_rate (eta):* Kleinere Werte machen das Training langsamer, aber stabiler. Ein niedrigerer Wert erfordert oft mehr n_estimators.\n",
    "- *n_estimators:* Gibt an, wie viele Bäume im Modell trainiert werden. Mehr Bäume können zu besseren Modellen führen, aber auch die Rechenzeit erhöhen.\n",
    "- *subsample:* Der Anteil der Trainingsdaten, die für jede Boosting-Runde zufällig ausgewählt werden. Reduziert Overfitting, Werte zwischen 0.5 und 1.0 sind üblich.\n",
    "- *colsample_bytree:* Der Anteil der Merkmale, die für das Training jedes Baumes zufällig ausgewählt werden. Reduziert Overfitting, indem es die Vielfalt der Bäume erhöht. Werte zwischen 0.5 und 1.0 sind üblich.\n",
    "- *reg_alpha (alpha):* L1-Regularisierungsterm, der eine Strafe für die Summe der absoluten Werte der Koeffizienten hinzufügt. Fördert Sparsamkeit im Modell, indem es einige Koeffizienten auf genau Null setzt, was effektiv einer Feature-Auswahl entspricht und Overfitting reduziert.\n",
    "- *reg_lambda (lambda):* L2-Regularisierungsterm, der eine Strafe für die Summe der Quadrate der Koeffizienten hinzufügt. Stabilisiert das Modell, indem es alle Koeffizienten schrumpft, um Überanpassung zu reduzieren, ohne sie auf Null zu setzen, was zu einem glatteren und weniger komplexen Modell führt.\n",
    "- *gamma:* Mindesteste Verlustreduktion, die erforderlich ist, um eine Baumaufspaltung durchzuführen. Höhere Werte führen dazu, dass weniger Splits durchgeführt werden, was zu einfacheren und weniger overfitted Modellen führt.\n",
    "\n",
    "Wie im Data Understanding bereits festgestellt wurde, ist die Zielvariable \"Attrition\" unausgeglichen ist. 1.233 (84%) Mitarbeitende haben das Unternehmen nicht verlassen, während nur 237 (16%) gegangen sind. XGBoost hat einen speziellen Parameter scale_pos_weight für unausgeglichene Klassen.\n",
    "- *scale_pos_weight:* Kontrolliert das  Gleichgewicht zwischen positiven und negativen Werten. Der Wert wird verwendet, um den Gradienten für die positive Klasse zu skalieren. Dies hat zur Folge, dass das Modell beim Training der positiven Klasse Skalierungsfehler macht und das Modell dazu ermutigt, diese zu stark zu korrigieren. Dies kann wiederum dazu beitragen, dass das Modell eine bessere Leistung erzielt, wenn es Vorhersagen für die positive Klasse trifft. Wenn es zu weit geht, kann es dazu führen, dass das Modell die positive Klasse besser vorhersagt, was jedoch zu einer schlechteren Leistung der negativen Klasse oder beider Klassen führt. Ein typischer Wert wäre sum(negative instances) / sum(positive instances).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search: Definiere Parametergrenzen\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.1, 0.5),\n",
    "    'subsample': uniform(0, 0.8),\n",
    "    'colsample_bytree': uniform(0, 1),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(0, 1),\n",
    "    'scale_pos_weight': randint(1,50)\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV starten, n_iter gibt an, wieviele Iterationen durchgeführt werden\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "random_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "print(f\"Best Parameter: {random_search.best_params_}\")\n",
    "print(f\"Best Score: {random_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search: Definiere Parameter, je mehr Parameter betrachtet werden sollen, desto länger dauert die Suche\n",
    "# Daher werden hier nicht alle Variablen betrachtet \n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [5, 7, 9],\n",
    "    'learning_rate': [0.1, 0.2, 0.3],\n",
    "    'n_estimators': [80, 100, 120],\n",
    "    'subsample': [0.4, 0.6, 0.8],\n",
    "    'colsample_bytree':[0.1, 0.2, 0.3],\n",
    "    'scale_pos_weight' :[4, 6, 8]\n",
    "    #'reg_alpha': [0, 0.01, 0.1, 1],\n",
    "    #'reg_lambda': [0, 0.01, 0.1, 1],\n",
    "    # 'gamma': [0, 0.1, 0.5, 1]\n",
    "\n",
    "}\n",
    "\n",
    "# Grid Search durchführen\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3)\n",
    "grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "\n",
    "print(f\"Best Parameter: {grid_search.best_params_}, BEst Score: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen und Trainieren des XGBoost-Modells mit angepassten Parametern\n",
    "xgb_model = XGBClassifier(objective='binary:logistic',\n",
    "                          max_depth=7, \n",
    "                          learning_rate=0.2, \n",
    "                          n_estimators=120,\n",
    "                          subsample=0.6,\n",
    "                          colsample_bytree=0.2,\n",
    "                          scale_pos_weight=6, \n",
    "                          eval_metric='logloss')\n",
    "# xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Vorhersagen mit dem Modell\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "xgb_y_pred = (xgb_preds > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, xgb_y_pred)\n",
    "plt.figure(figsize=(4.5, 3))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Stay', 'Quit'], yticklabels=['Stay', 'Quit'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je höher der Parameter scale_pos_weight gesetzt wird, um so mehr verbessert sich die Vorhersage für die positive Klasse. Trotz eines relativ hohen scale_pos_weight, ist die Vorhersagegüte für die positive Klasse (Quit) weiterhin nicht gut. Im Testdatensatz wurde nur knapp die Hälfte der Personen, die das Unternehmen tatsächlich verlassen korrekt vorhergesagt. Bei etwas mehr als der Hälfte wurde hingegen vorhergesagt, dass sie das Unternehmen nicht verlassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(xgb_model, X_resampled, y_resampled, X_test, y_test, train=True )\n",
    "evaluation(xgb_model, X_resampled, y_resampled, X_test, y_test, train=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch die angepasste Parameterkonfigurationen hat sich der Accuracy Score zwar von 86,41 % auf 83,15 % verschlechtert. Allerdings wurde der Recall-Score (misst die Fähigkeit, alle positiven Fälle zu finden (Verhältnis positiver Vorhersagen zu tatsächlich positive Werten)) von 37,5 % auf 47,9 % verbessert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4 Visualisierung der Modellperformance\n",
    "\n",
    "Die *ROC-Curve* (Receiver Operating Characteristic) ist ein Tool zur Bewertung der Leistung eines binären Klassifikationsmodells. Sie stellt die wahre positive Rate (True Positive Rate, TPR) gegenüber der falschen positiven Rate (False Positive Rate, FPR) dar, um die Trennschärfe des Modells bei verschiedenen Schwellenwerten zu visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnen der FPR, TPR und AUC\n",
    "fpr_xgb_model, tpr_xgb_model, _ = roc_curve(y_test, xgb_preds)\n",
    "roc_auc_xgb_model = roc_auc_score(y_test, xgb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotten der ROC-Kurve für XGBoost\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(fpr_xgb_model, tpr_xgb_model, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_xgb_model:0.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - XGBoost')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5 Kreuzvalidierung\n",
    "\n",
    "Mit Kreuzvalidierung kann geprüft werden, wie gut ein Modell auf neue Daten angewendet werden kann. Dafür wird das Modell k-mal trainiert mit unterschiedlich großen Teilmengen. Die Modelleistung wird für jeden Durchlauf gemessen und ein Durchschnitt berechnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kreuzvalidierung\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "cross_val_results = cross_val_score(xgb.XGBClassifier(objective='binary:logistic',\n",
    "                          max_depth=7, \n",
    "                          learning_rate=0.2, \n",
    "                          n_estimators=120,\n",
    "                          subsample=0.6,\n",
    "                          colsample_bytree=0.2,\n",
    "                          scale_pos_weight=6, \n",
    "                          eval_metric='logloss'), X, y, cv=kfold, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy: {cross_val_results.mean()} +/- {cross_val_results.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.6 Vergleich Modell-Performance mit Daten im Originalformat vs. bereinigte Daten (One-Hot-Encoding)\n",
    "\n",
    "Verbessert sich die ROC-Curve, wenn XGBoost auf den Originaldaten (Datentyp Object, der für XGBoost in kategoriale Variablen umgewandelt wird) anstatt auf den mittels One-Hot Encoding bereinigten Daten ausgeführt wird?\n",
    "\n",
    "Hierfür wird der Datensatz employee_data_transformed benötigt. Dieser wird auf die gleiche Weise aufgeteilt in Trainings- und Testdaten wie der Datensatz employee_data (s.o.). Außerdem wird auch in diesem Datensatz mitttels SMOTE eine Gleichgewichtung der Zielvariablen erzeugt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datenvorbereitung\n",
    "categorical_col = []\n",
    "for column in employee_data_transformed.columns:\n",
    "    if employee_data_transformed[column].dtype == object:\n",
    "        categorical_col.append(column)\n",
    "for column in categorical_col:\n",
    "    employee_data_transformed[column] = employee_data_transformed[column].astype(\"category\").cat.codes\n",
    "employee_data_transformed['Attrition'] = employee_data_transformed['Attrition'].astype(\"category\").cat.codes\n",
    "\n",
    "#Aufteilen der Daten in Zielvariable X_2 und Attribute y_2.\n",
    "X_2 = employee_data_transformed.drop('Attrition', axis=1).values  # Merkmale als NumPy-Array\n",
    "y_2 = employee_data_transformed['Attrition'].values               # Zielvariable als NumPy-Array\n",
    "\n",
    "#Aufteilen der Daten in Test und Trainingsdaten.\n",
    "X_2_train, X_2_test, y_2_train, y_2_test = train_test_split(X_2, y_2, test_size=0.25, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_2_resampled, y_2_resampled = smote.fit_resample(X_2_train, y_2_train)\n",
    "\n",
    "# Zählen der Anzahl der \"Yes\"-Labels (Attrition = 1) vor SMOTE\n",
    "num_yes_before = np.sum(y_2_train == 1)\n",
    "print(f\"Anzahl der 'Yes' (Attrition = 1) vor SMOTE: {num_yes_before}\")\n",
    "num_yes_after = np.sum(y_2_resampled == 1)\n",
    "print(f\"Anzahl der 'Yes' (Attrition = 1) nach SMOTE: {num_yes_after}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = employee_data_transformed.drop('Attrition', axis=1) # Zielvariable entfernen\n",
    "y_2 = employee_data_transformed['Attrition']  # 'Attrition' Spalte als Series\n",
    "\n",
    "# Train-Test-Split durchführen\n",
    "X_2_train, X_2_test, y_2_train, y_2_test = train_test_split(X_2, y_2, test_size=0.25, random_state=42)\n",
    "\n",
    "# Textfeatures in Kategorien umwandeln\n",
    "categories = X_2_train.select_dtypes(include=object).columns.tolist()\n",
    "for col in categories:\n",
    "    X_2_train[col] = X_2_train[col].astype('category')\n",
    "    X_2_test[col] = X_2_test[col].astype('category')\n",
    "\n",
    "# Labels in numerische Codes umwandeln\n",
    "if y_2_train.dtype == 'object':\n",
    "    y_2_train = y_2_train.astype('category')\n",
    "    y_2_train = y_2_train.cat.codes\n",
    "\n",
    "if y_2_test.dtype == 'object':\n",
    "    y_2_test = y_2_test.astype('category')\n",
    "    y_2_test = y_2_test.cat.codes\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_2_resampled, y_2_resampled = smote.fit_resample(X_2_train, y_2_train)\n",
    "\n",
    "# Zählen der Anzahl der \"Yes\"-Labels (Attrition = 1) vor SMOTE\n",
    "num_yes_before = np.sum(y_2_train == 1)\n",
    "print(f\"Anzahl der 'Yes' (Attrition = 1) vor SMOTE: {num_yes_before}\")\n",
    "num_yes_after = np.sum(y_2_resampled == 1)\n",
    "print(f\"Anzahl der 'Yes' (Attrition = 1) nach SMOTE: {num_yes_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen der DMatrix\n",
    "# DMatrix ist eine spezielle Datenstruktur, die von XGBoost verwendet wird.\n",
    "# \"enable_categorial=True\" ermöglich die Verarbeitung kategorialer Daten\n",
    "\n",
    "dtrain_reg = xgb.DMatrix(X_2_resampled, y_2_resampled, enable_categorical=True)\n",
    "dtest_reg = xgb.DMatrix(X_2_test, y_2_test, enable_categorical=True)\n",
    "\n",
    "# Modell trainieren\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 7,\n",
    "    'eta': 0.2,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree':0.2,\n",
    "    'scale_pos_weight':6,\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "xgb_model_2 = xgb.train(params, dtrain_reg, num_boost_round=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorhersagen\n",
    "y_pred_prob_2 = xgb_model_2.predict(dtest_reg)\n",
    "y_pred_2 = (y_pred_prob_2 > 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "accuracy = accuracy_score(y_2_test, y_pred_2)\n",
    "mse = mean_squared_error(y_2_test, y_pred_prob_2)\n",
    "rmse = np.sqrt(mse)\n",
    "auc = roc_auc_score(y_2_test, y_pred_prob_2) \n",
    "fpr_xgb_model_2, tpr_xgb_model_2, _ = roc_curve(y_2_test, y_pred_prob_2)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"mse: {mse}\")\n",
    "print(f\"rmse: {rmse}\")\n",
    "print(f\"Area under Curve: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotten der ROC-Kurve für XGBoost\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(fpr_xgb_model, tpr_xgb_model, color='darkorange', lw=2, label=f'ROC curve encoded (area = {roc_auc_xgb_model:0.4f})')\n",
    "plt.plot(fpr_xgb_model_2, tpr_xgb_model_2, color='darkgreen', lw=2, label=f'ROC curve original (area = {auc:0.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - XGBoost')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.7 Visualisierung der Feature Importance und des Entscheidungsbaums\n",
    "\n",
    "Der Feature Importance Plot berechnet, wie häufig das Feature verwendet wurde, um Entscheidungsbäume im Modell aufzuteilen. Nachfolgend wird die unterschiedliche Gewichtung der Features in Abhängigkeit der zugrunde liegenden Daten (Encoded oder original) dargestellt.\n",
    "\n",
    "EmployeeNumber und Monthly Income sind in beiden Fällen sehr wichtig für die Aufteilung der Entscheidungsbäume. Beim Encoded Datensatz führt mit Abstand das MonthlyIncome, beim Original Datensatz mit Abstand die EmployeeNumber das Ranking an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb.plot_importance(xgb_model, importance_type=\"weight\", max_num_features =10, title='Feature Importance - Encoded Data')\n",
    "plt.show()\n",
    "\n",
    "xgb.plot_importance(xgb_model_2, importance_type=\"weight\", max_num_features =10, title='Feature Importance - Original Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Entscheidungsregeln und die Struktur der Bäume besser zu verstehen, kann man sich diese grafisch darstellen lassen. Bei einer maximalen Tiefe von 7 wird der Baum allerdings schon sehr unübersichtlich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot des Entscheidungsbaumes\n",
    "fig, ax = plt.subplots(figsize=(80,50)) \n",
    "xgb.plot_tree(xgb_model, num_trees=0, rankdir = 'LR',  ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Support Vector Machine (SVM) ist ein überwachter Lernalgorithmus, der versucht das Risiko von Overfitting zu verringern. Bei der SVM wird jedes Mitglied des Trainingsdatensatzes einer vor zwei Kategorien zugewiesen, sodass es sich um einen nicht-probabilistischen binären linearen Klassifikator handelt. Bei einer linearen Klassifikation werden die Eingabetrainingsdatenpunkte in einem Raum (Hyperplane) abgebildet, jeder mit einem anderen Klassenlabel, das durch eine klare Lücke voneinander getrennt ist. Neue Datenpunkte werden dann in denselben Raum abgebildet und als Teil einer Klasse vorhergesagt, je nachdem, auf welcher Seite der Lücke sie liegen. SVM kann aber auch nicht-lineare Klassifikationen durchführen. Nebst der Robustheit gegenüber Überanpassung liegt der Vorteil in der Effektivität in hochdimensionalen Räumen (also wenn die Anzahl der Merkmale größer als die Anzahl an Beobachtungen ist). Durch verschiedene Kernel-Funktionen (darunter linear, polynominal und sigmoid) sind SVMs flexibel auf verschiedene Datensätze und Problemtypen einsetzbar. Allerdings gestaltet sich die Modellauswahl mitunter durch das Hyperparameter-Tuning und die Kernel-Auswahl als komplex. Unter der Modellkomplexität leidet ebenfalls die Interpretierbarkeit (Cortes und Vapnik 1995)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen und trainieren des SVM Modells \n",
    "svm = SVC(kernel='rbf', C=1.0, probability=True, random_state=42)\n",
    "svm.fit(X_resampled, y_resampled)\n",
    "evaluation(svm, X_resampled,  y_resampled, X_test, y_test, train=True)\n",
    "evaluation(svm, X_resampled,  y_resampled, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verwendeter Kern: rbf - Auch als Gaussian Kernel bezeichnet, misst er die Ähnlichkeit der Datenpunkte, indem er den euklidischen Abstand in den exponentiellen Term einbezieht. Das dieser Kern besonders effektiv bei komplexen und nichtlinearen Mustern ist, ist er im Falle dieses Datensatzes geeignet. \n",
    "\n",
    "PCA wird angewendet, um die Dimensionen des hochdimensionalen HR-Datensatzes zu reduzieren, der 40 Merkmale umfasst. Diese Technik transformiert die Daten in einen kleineren Raum (z. B. 2D), indem sie die Hauptkomponenten wählt, die die meiste Varianz erklären. Dies erleichtert die Visualisierung und Interpretation des Modells, insbesondere bei der Darstellung von Entscheidungsgrenzen des SVM-Modells in zwei Dimensionen. Außerdem hilft PCA, irrelevante oder korrelierte Merkmale zu entfernen, was die Effizienz und Performance des Modells verbessern kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_resampled) \n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o')\n",
    "plt.xlabel('Anzahl der Hauptkomponenten')\n",
    "plt.ylabel('Kumulierte erklärte Varianz')\n",
    "plt.title('Varianz erklärt durch PCA')\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label='90% Varianz')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA-Komponenten und ihre Bedeutung für die ursprünglichen Features\n",
    "components = pca.components_\n",
    "features = X_resampled.columns  # Namen der Features\n",
    "\n",
    "# Ausgabe der ersten zwei Hauptkomponenten\n",
    "for i in range(2):  # oder die Anzahl der gewünschten Komponenten\n",
    "    component = components[i]\n",
    "    feature_importance = sorted(zip(features, component), key=lambda x: abs(x[1]), reverse=True)\n",
    "    print(f\"Hauptkomponente {i + 1}:\")\n",
    "    for feature, importance in feature_importance:\n",
    "        print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionsreduktion mit PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_resampled)\n",
    "\n",
    "# Trainieren des SVM-Modells auf den reduzierten Daten\n",
    "svm_pca = SVC(kernel='rbf', C=1.0, probability=True, random_state=42)\n",
    "svm_pca.fit(X_pca, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Entscheidungsgrenzen\n",
    "def plot_decision_boundary(X, y, model, title='Entscheidungsgrenze'):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 8),\n",
    "                         np.arange(y_min, y_max, 8))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.viridis)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', cmap=plt.cm.viridis)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Merkmal 1')\n",
    "    plt.ylabel('Merkmal 2')\n",
    "    plt.show()\n",
    "\n",
    "# Plot der Entscheidungsgrenzen\n",
    "plot_decision_boundary(X_pca, y_resampled, svm_pca, title='Entscheidungsgrenzen des SVM auf PCA-reduzierten Daten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(svm_pca, X_pca,  y_resampled, X_test, y_test, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das PCA-Reduzierte Datenset verbessert die Ergebnisse des Modells nicht.  \n",
    "Mithilfe von GridSearch wird nun eine geeignete Parameterkombination identifiziert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(random_state=42)\n",
    "\n",
    "param_grid = [\n",
    "    {'C': [0.1, 1, 6, 10], 'gamma': [0.001, 0.005, 0.01], 'kernel': ['rbf']},\n",
    "]\n",
    "\n",
    "\n",
    "search = GridSearchCV(svm, param_grid=param_grid, scoring='roc_auc', cv=3, refit=True, verbose=1)\n",
    "search.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei mehrmaligem Testen mit unterschiedlichen RandomStates wird deutlich, dass die geeigneten Parameterkombinationen stark voneinander abweichen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(**search.best_params_, probability=True)\n",
    "svm.fit(X_resampled, y_resampled)\n",
    "\n",
    "evaluation(svm, X_resampled,  y_resampled, X_test, y_test, train=True)\n",
    "evaluation(svm, X_resampled,  y_resampled, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualisierung der Modellperformance\n",
    "\n",
    "Die *ROC-Curve* (Receiver Operating Characteristic) ist ein Tool zur Bewertung der Leistung eines binären Klassifikationsmodells. Sie stellt die wahre positive Rate (True Positive Rate, TPR) gegenüber der falschen positiven Rate (False Positive Rate, FPR) dar, um die Trennschärfe des Modells bei verschiedenen Schwellenwerten zu visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_svm, tpr_svm, _ = roc_curve(y_test, svm.predict(X_test))\n",
    "roc_auc_svm = roc_auc_score(y_test, svm.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotten der ROC-Kurve für SVM\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(fpr_svm, tpr_svm, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_svm:0.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve SVM')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleich der Methoden \n",
    "Im Folgenden werden die zuvor angewendeten Modelle SVM, XGBoost und Random Forest verglichen um zu bewerten, welches Modell die besten Ergebnisse liefert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotten der ROC-Kurven der verschiedenen Modelle\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# ROC-Kurve vor dem Tuning\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(fpr_svm, tpr_svm, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_svm:0.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve SVM')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# ROC-Kurve XGBoost\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(fpr_xgb_model_2, tpr_xgb_model_2, color='darkorange', lw=2, label=f'ROC curve original (area = {auc:0.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC XGBoost')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# ROC-Kurve Random Forest\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(fpr_rf, tpr_rf, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_rf:0.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Random Forest')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erklärung der Vorhersagen mittels SHAP\n",
    "\n",
    "SHapley Additive exPlanations (SHAP) ist eine Methode zur Erklärung der Vorhersagen von Machine-Learning-Modellen. In XGBoost wird SHAP verwendet, um die Beitrage einzelner Merkmale zu den Modellvorhersagen zu quantifizieren und zu visualisieren. \n",
    "\n",
    "SHAP-Werte quantifizieren den Einfluss jedes Merkmals auf die Modellvorhersage.\n",
    "Mit SHAP können verschiedene Plots erstellt werden, um die Bedeutung und den Einfluss der Merkmale zu visualisieren.\n",
    "\n",
    "- Im *Summary Plot* sind die Merkmale nach Bedeutung sortiert. Die X-Achse zeigt den Einfluss des Merkmals auf die Vorhersage. \n",
    "Postive Werte bedeuten, dass das Merkmal die Wahrscheinlichkeit erhöht, dass das Modell eine positive Klasse vorhersagt. Negative Werte bedeuten, dass das Merkmal diese Wahrscheinlichkeit verringert. \n",
    "Die Farbe repräsentiert den Wert des Merkmales (rot hohe Werte, blau niedrige Werte)\n",
    "\n",
    "- Ein *Dependence Plot* zeigt die Beziehung zwischen den SHAP-Werten eines bestimmten Merkmals und den Werten dieses Merkmals selbst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellung SHAP explainer\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Summary Plot\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "# Dependence plot für das Merkmal \"Monthly Income\"\n",
    "shap.dependence_plot('MonthlyIncome', shap_values, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
